---
title: "Propensity Score Methods"
author: "Sereina M. Graber"
date: "2022-12-16"
css: "../styles_blog_individual.css"
image: propensity-featured.jpg
---

```{r}
#| include: false
#| label: setup


# packages
#library(ExaInOut)
library(DescTools)
library(knitr)
library(kableExtra)
library(tidyverse)
library(broom)
library(magrittr)
library(data.table)
library(Rmisc)
library(plotly)
library(directlabels)
#library(patchwork)
library(gtsummary)
library(summarytools)
library(formattable)
library(rstudioapi)
library(DT)
library(pROC)
library(ggthemes)
library(colorspace)
library(locfit)
library(epitools)

library(nnet)
library(MASS)
library(multcomp)
library(survey)

library(circlize)
library(here)

library(MatchIt)
#theme_gtsummary_journal()
theme_gtsummary_compact() 
#> Setting theme `Compact`, reduces font size and cell padding
```

```{r}
#| include: false
#| label: functions


# ==============================================================================
OR_CI_fct <- function(model, variable){
  
  OR_CI_all <- DescTools::OddsRatio(model, use.profile = FALSE, digits = 5)$or
  OR_CI <- tibble::rownames_to_column(data.frame((OR_CI_all))) %>% dplyr::filter(rowname == variable) %>% dplyr::select(or, or.lci, or.uci)
  
  return(OR_CI)
  
}



# (Standardized) DIFFERENCE IN MEANS OR PROPORTIONS (binary variables) =========
# for checking balance in covariate after propensity score methods are applied
mean.prop.diff <- function(data, variable, treatment, weights = NULL){
  
  if(is.null(weights)){
    # Renaming the columns for convenience 
    # (i.e. not using {{ }} afterwards)
    data <- data %>% 
              mutate(weights_var = 1) %>% 
              rename(variable_var := {{variable}},
                     treatment_var := {{treatment}}) %>% 
              mutate(treatment_var = as.factor(treatment_var))
    message("No weights given, mean/proportion differences are unweighted!")
    
  }else{
    # Renaming the columns for convenience 
    # (i.e. not using {{ }} afterwards)
    data <- data %>% 
              rename(variable_var := {{variable}},
                     treatment_var := {{treatment}},
                     weights_var := {{weights}}) %>% 
              mutate(treatment_var = as.factor(treatment_var))
    message("Weights given, mean/proportion differences are weighted!")
  }
  
  control_level <- levels(data$treatment_var)[1]
  treatment_level <- levels(data$treatment_var)[2]
  message(paste0("'", treatment_level, "' is used as treatment level!"))
  
  m_treat <- data %>% 
               filter(treatment_var == treatment_level) %>%
               summarise(m = weighted.mean(variable_var, weights_var)) %>%
               pull(m)
                
  m_control <- data %>% 
                filter(treatment_var == control_level) %>% 
                summarise(m = weighted.mean(variable_var, weights_var)) %>% 
                pull(m)
  
  diff <- m_treat - m_control
  
  # for continuous variables 
  if(class(data$variable_var) == 'numeric'){
    
    var_treat <- data %>% 
                    filter(treatment_var == treatment_level) %>%
                    summarise(v = Hmisc::wtd.var(variable_var, weights = weights_var)) %>%
                    pull(v)
    
    var_control <- data %>% 
                    filter(treatment_var == treatment_level) %>%
                    summarise(v = Hmisc::wtd.var(variable_var, weights = weights_var)) %>%
                    pull(v)
    
    std_mean_diff_att <- diff / sqrt(var_treat) # for ATT
    std_mean_diff_ate <- diff / sqrt(var_treat + var_control/2) # for ATE
    
    return(list(mean_diff = diff,
                std_mean_diff_ATT = std_mean_diff_att,
                std_mean_diff_ATE = std_mean_diff_ate))
    
    
  }else{ # for binary (dummy coded categorical) variables 
  
    std_prop_diff_att <- diff / sqrt(m_treat * (1-m_treat)) # ATT
    std_prop_diff_ate <- diff / sqrt( ((m_treat * (1-m_treat)) + (m_control * (1-m_control)) ) / 2 )
    
    return(list(prop_diff = diff, 
                std_prop_diff_ATT = std_prop_diff_att, 
                std_prop_diff_ATE = std_prop_diff_ate)
           )
  }
  
  
}



# Mark cells red in case > or < 0.05 (does not allow NAs) ======================
red_cell_spec <- function(x){
  kableExtra::cell_spec(round(x, 6), color = ifelse(x > 0.05 | x < -0.05, "red","black"))
  }



# ==============================================================================

pairwise.balance <- function(condition1, condition2, data, treat_var, iptw, covariates){
  
  data %<>% 
    rename(treatment_var := {{treat_var}},
           iptw := {{iptw}}) %>% 
    dplyr::select(treatment_var, iptw, all_of(covariates))
  
  data <- subset(data, treatment_var == condition1 | treatment_var == condition2)
  
  data$treatment_var <- as.numeric(data$treatment_var == condition1)
  
  balance.iptw <- twang::bal.stat(data = as.data.frame(data), # does not allow tibbles
                                  vars = covariates, 
                                  treat.var = 'treatment_var' ,
                                  w.all = unlist(data[, 'iptw']), # does not allow tibbles -> unlist
                                  get.ks = F, 
                                  sampw = 1,
                                  estimand='ATT',
                                  multinom = F)
  
  results <- rownames_to_column(balance.iptw$results)
        
  return(results[, c('rowname', 'std.eff.sz')]) 
  
}
```

<br><br>

# Rationale

In case randomized controlled trials (RCTs) are not feasible due to ethical and/or practical reasons, cause-effect relationships are often examined based on empirical data in observational studies. Because in observational studies, the study subjects are not assigned by random, the groups often show systematic differences, which need to be accounted for. To do so, there are basically two methodological approaches used: adjustment based on multiple regression analyses or propensity score methods.

<br>

::: columns
::: {.column width="47.5%"}
**Randomized controlled trial (RCT)**

-   goal: clarification of cause-effect relationship
-   an intervention/ treatment is randomly assigned to study subjects
-   intervention and control group show no systematc differences and can be compared directly --\> unbiased effects
:::

::: {.column width="5%"}
<!-- empty column to create gap -->
:::

::: {.column width="47.5%"}
**Observational study**

-   empirical investigation to elucidate cause-effect relationships
-   ...under circumstances where it is impossible to perform a controlled experiment
-   study subjects in the groups under study often show systematic differences --\> biased effects
-   cause-effect relationship cannot be examined without considering covariates
:::
:::

<br>

```{r}
#| include: false

# ---------------------------------------------
# further things which could be elaborated on
# ---------------------------------------------

# "For each subject, the effect of treatment is defined to be $Y_{i}(1) - Y_{i}(0)$ The *average treatment effect* (ATE) is defined to be $E[Y_{i}(1) - Y_{i}(0)]$ (Imbens, 2004). The ATE is the average effect, at the population level, of moving an entire population from untreated to treated. A related measure of treatment effect is the average treatment effect for the treated (ATT; Imbens, 2004). The ATT is defined as $E[Y(1) - Y(0 | Z = 1)]$ (Z = indicator variable denoting the treatment received). The ATT is the average effect of treatment on those subjects who ultimately received the treatment. In an RCT these two measures of treatment effects coincide because, due to randomization, the treated population will not, on average, differ systematically from the overall population." (Austin 2011).


#In case of an RCT, ATT = ATE. In case of observational studies these values are not necessarily the same (see [here](https://economics.stackexchange.com/questions/45335/what-is-the-difference-between-ate-and-att)). For simplicity, this article focuses on the ATT. The weights for the IPTW as well as standardized mean difference, however, depend on whether one intends to estimate the ATT or an ATE.


#Conditional vs marginal effects: see e.g.: https://cran.r-project.org/web/packages/MatchIt/vignettes/estimating-effects.html
```


What I consider as very helpful sources covering specifically propensity score matching and weighting are the package documentations of [MatchIt](https://kosukeimai.github.io/MatchIt/) and [WeightIt](https://ngreifer.github.io/WeightIt/index.html), as well as further references mentioned throughout this post.
With the following descriptions I tried to give my future self an overview of the different methods in my own words, with less emphasis on the theory and more on the practice. The methods described here, however, are not exhaustive. The analyses given show an attempt to compare the different methods and approaches of adjusting for baseline characteristics, using data from a retrospective observational study.

<br><br>

# Case study

As an illustrative purpose we can use the `lindner` data set from package `PSAgraphics`. The data contains information on almost 1000 patients who underwent Percutaneous Coronary Interventions (PCIs), a non-surgical procedure to improve the blood flow through narrowed or blocked coronary arteries. 
Patients underwent either the usual PCI, or the usual PCI augmented by planned or rescue

People in group A are very specific , and we suspect using adjustment via traditional multiple regression procedure does not appropriately balance the differences in baseline characteristics between the two groups . Therefore different propensity score methods are applied and are compared to the traditional multiple regression analysis approach.

```{r data}
#| collapse: true

# load necessary package
library(PSAgraphics)
data("lindner")

# data set
str(lindner)
```

In the given case study `abcix` represents the dichotomous "treatment" variable with two levels, B as the control group and A as the treatment group.
```{r}
treatment <- 'abcix'
covariates <- c('stent', 'height', 'female', 'diabetic', 'acutemi', 'ejecfrac', 'ves1proc')
```




<br><br>

# R packages

-   `MatchIt`: 

    -   various type of matching
    -   stratification
    -   for dichotomous treatment variables only
    -   package documentation: [here](https://kosukeimai.github.io/MatchIt/).

-   `WeightIt`: 

    - weighting 
    - package documentation: [here](https://ngreifer.github.io/WeightIt/index.html).

-   `cobalt`:

    -   computing & plotting balance statistics

-   `twang`:

    -   weighting
    -   computing & plotting balance statistics
    -   propensity score estimation for multiple treatments

<br><br>

# Propensity Score Estimation: theory

All propensity score methods do somehow involve the **propensity score**. The propensity score is represented by the probability \[0, 1\] of a person or subject belonging to the intervention group, based on measured baseline characteristics (\~ covariates). The scores are often estimated based on a logistic regression model (also other methods are feasible) regressing the dichotomous treatment variable against the baseline characteristic of interest. The model is also know as the *propensity score model*. In the given case study, the propensity score model regresses the dichotomous treatment against the "pre-treatment" baseline characteristics. "Pre-treatment" refers to characteristics which affect being part of A or B, but not the other way around, so these characteristics should not be subsequently influenced by the treatment group.

$$
\begin{equation}
p(A) = propensity \ score = age + sex + bla + bla + bla + ... 
\end{equation}
$${#eq-zero}
The propensity score is used as a "balancing score" to equalize baseline characteristics between the groups being compared.

To achieve a good balance between the treatment groups, it is desirable that the propensity scores of the groups have a large overlap. In this context, however, I asked myself what role the model performance (predictive ability) of the propensity score model (e.g. AUC) plays here. Here, the literature recommends that the propensity score models should not primarily be optimized based on the predictive ability, but rather based on the ability to balance the groups as well as possible. More on this here: [Stack Exchange](https://stats.stackexchange.com/questions/455027/why-dont-people-report-the-accuracy-ppv-or-npv-of-their-propensity-score-mode/).

Furthermore, there are quite some studies which investigated what kind of variables should be included in the propensity score model, however, in general there is no clear recommendation of what those are. A couple of studies have shown, though, that in addition to true confounders (affecting both, treatment and outcome variable), also variables that only influence the outcome but not treatment should be included. In practice, however, this is often difficult to classify.

<br>

# Propensity Score Methods: theory

The three most commonly used propensity score methods are (1) propensity score *matching*, (2) propensity score *weighting* (inverse probability of treatment weighting (IPTW)) and (3) propensity score *stratification*. These are briefly described below:

<br>

::: columns
::: {.column width="33.333333%"}
**Propensity Score Matching**

-   preprocessing data in a way that no dependence between intervention and baseline covariates
-   simplest (amongst others) 1:1 nearest neighbor matching based on propensity scores. To each person in the intervention group match one person from the control group
    -   with/ without replacement
    -   greedy/ optimal matching
-   R package: `MatchIt`
:::

::: {.column width="33.333333%"}
**Propensity Score Weighting**

-   pseudopopulation in which gropu membership (intervention vs. control) is to be made independent of baseline covariates
-   weighting of individuals based on propensity scores:
    -   intervention group: $\frac{1}{ps}$
    -   control group: $\frac{1}{1 - ps}$
-   R package: `weightIt`
:::

::: {.column width="33.333333%"}
**Propensity Score Stratification**

-   stratification based on propensity scores (e.g. quintiles)
-   per propensity score stratum group differences evaluated
-   averaged group effect across all strati
-   often, stratum-specific effects are weighted by number of individuals in a given stratum
-   R package: `MatchIt`
:::
:::

<br><br>

# Assessment of comparability between groups: theory

Once a propensity score model has been defined and a method (matching, weighting or stratificaiton) has been applied, the next crucial step is to check on how well the compared groups are balanced in terms of the covariates. In case the differences are still too big, the process of propensity score estimation should be repeated (e.g. using non-linear effects) until an adequate balance is achieved.

One very helpful measure to compare the means/ proportion of the two groups under study is the *standardized mean/ proportion difference*. This can be calculated for continuous as well as binary variables. Its exact calculation depends on the estimand ATT or ATE (\~ `estimand` argument in the `matchit()` function, for more details please see `?summary.matchit`).

Standardized mean difference for *continuous* variables:

$$
\begin{equation}
\Large d = \frac{\bar{x}_{treatment} - \bar{x}_{control}}{\sqrt{s^{2}_{treatment}}} 
\end{equation}
$$ {#eq-one} <br>

Standardized proportion difference for *binary* variables (categorical variables dummy coded):

$$
\begin{equation}
\Large d = \frac{(p_{treatment} - p_{control})}{\sqrt{p_{treatment}(1 - p_{treatment})}} 
\end{equation}
$$ {#eq-two}

<br><br>

# Propensity score *matching* using `MatchIt`: application

There are different ways of how treatment subjects can be matched to control subjects.

-   **exact matching**: treatment and control subjects are matched based the exact same baseline characteristics. In case there are many covariates and especially in case variables take many different values, it is very often almost impossible to find for each subject from the intervention group an exact match in the control group.

-   **nearest neighbour matching**: this is the *default* setting in the `matchit()` function and matched to each individual in the treatment group the *r* closest invividuals from the control group, based on a given distance measure.

-   **optimal matching**: when using nearest neighbour matching, the default is the so called "greedy" method. In this method a treatment individual is randomly select and the closest control is matched, regardless of whether the chosen individual from the control group would be closer to another individual in the treatment group. *Optimal* matching minimizes the average pairwise differences between treatment and control group.

There are a couple of other methods available, which are not further discussed here. For more detailed information please refer to [Ho et al. 2011](https://www.jstatsoft.org/article/view/v042i08).

 <br>

Here simply a 1:1 nearest neighbour matching us used:

```{r}
# matchit object using the nearest neighbour matching method
# res.nearest <- MatchIt::matchit(versmodel_a ~ age_n + sex_c + sprgeb_x + pre_bl_b + pre_kons_n + pre_stat_fg + pre_atc_amb_cat + pre_pcg_cat,
#                               data = d.tele.roh, 
#                               method = 'nearest',
#                               estimand = 'ATT') # ATT (default) = average treatment effect on the treated

# d.matchit <- MatchIt::match.data(res.nearest)
```

The propensity scores can be extracted for the whole sample via `res.nearest$distance` and they can be plotted automatically using the `bal.plot()` function from the package `cobalt`:

```{r, fig.width = 7, fig.height = 3.7, fig.align = 'center', warning=FALSE}
# cobalt::bal.plot(res.nearest, var.name = 'distance', which = 'both')
```

Calling `summary()` on the output of the matching procedure gives various measures to assess the differences or balance in the covariates between the treatment (A) and the control group (B), for the matched as well as for the unmatched data.

`summary()` on the *matchit*-object:

```{r}
# smry.nearest <- summary(res.nearest)
```

<br>

Number of *matched* and *unmatched* units:

```{r}
# smry.nearest$nn
```

<br>

Group comparison of *unmatched* data (only first 3 columns of `summary`- output):

```{r}
# smry.nearest$sum.all[, 1:3]
```

<br>

Group comparison of *matched* data (only first 3 columns of `summary`- output):

```{r}
# smry.nearest$sum.matched[, 1:3]
```

To extract the matched data set one can use the following line of code `d.nearest <- match.data(res.nearest)`.

The main downside of using propensity score *matching* is that one looses - depending on how many treatment subjects there are - quite many of the control subjects. Depending on circumstances, this can even lead to limited representativeness if subjects with extreme propensity scores get excluded. Therefore, propensity score *weighting* might present a valid alternative.

<br><br>

# Propensity score *weighting*: application {#weighting}

Inverse probability of treatment weighting (IPTW). Estimation differs whether one aims to esimate the ATE or ATT.

First, lets have a look again at the differences in covariates between A and B group based on the standardized mean differences and the differences in proportions, respectively, this time using the `bal.tab()` function from the `cobalt` package. This functions generate 'balance' statistics for the given covariates, and since no matching or weighting is happening so far, these results refer to the differences in the raw data. This should basically result in the same output as for the raw data using the `summary()` function on the *matchit*-object from above (i.e. `smry.nearest$sum.all[, 1:3]`):

```{r}
# similar to when using summary(matchit(, estimand = 'ATT'))
# cobalt::bal.tab(versmodel_a ~ age_n + sex_c + sprgeb_x + pre_bl_b + pre_kons_n + pre_stat_fg + pre_atc_amb_cat + pre_pcg_cat, data = d.tele.roh, estimand = 'ATT', thresholds = c(mean.diffs = 0.05))
```

The standardized mean differences for continuous variables are identical to the above results of the raw data, but the difference measure for categorical variables deviate. Whereas based on the `MatchIt` package the equation \@ref(eq:two) is used, the `bal.tab()` function simply calculates the difference in proportions $p_{treatment} - p_{control}$ without standardizing.

<br>

## Propensity score *weighting* using `WeightIt` and `cobalt`

The inverse probability weights based on propensity scores can be generated using the `weightit()` function from the package `WeightIt`:

```{r}
# res.weightit <- WeightIt::weightit(versmodel_a ~ age_n + sex_c + sprgeb_x + pre_bl_b + pre_kons_n + pre_stat_fg + pre_atc_amb_cat + pre_pcg_cat, data = d.tele.roh, estimand = 'ATT', method = 'ps')
```

Based on this output, the propensity scores and the inverse probability weights can easily be extracted using `res.weightit$ps` and `res.weightit$weights`, respectively.

Using `summary()` on the *weightit*-object provides various quality or variability measures on the weights. Good weights are generally characterized by low variability, since this leads to improved estimators.

```{r}
# summary(res.weightit)
```

Since we use the ATT estimand, the inverse probability weights are only applied on the control group, whereas for the treatment group (\~tele) all weights are equal 1.

The balance statistics for the *weighted* data can be extracted using again the `bal.tab()` function from the `cobalt` package:

```{r}
# cobalt::bal.tab(res.weightit, stats = c('mean.diffs'), thresholds = c(mean.diffs = 0.05))
```

Please note again here, that in case of binary coded variables the `bal.tab()` function simply calculates the difference in proportions $p_{treatment} - p_{control}$ without standardizing. Besides that, the weighted data looks quite balanced between the treated (\~ A) and the control (\~ B) group.

<br>

## Propensity score *weighting* "by hand"

Once the propensity score are calculated, getting the weights for the IPTW procedure is quite easy, and therefore one does not necessarily need a predefined function/ package. Also, doing it 'manually' allows for more flexibility in the propensity score model and applied weights.

Define propensity score model and generate the weights for the ATT estimand:

```{r}
# prop.score.model <- glm(versmodel_a ~ age_n + sex_c + sprgeb_x + pre_bl_b + pre_kons_n + pre_stat_fg + pre_atc_amb_cat + pre_pcg_cat, data = d.tele.roh, family = 'binomial')
# 
# d.weights <- d.tele.roh %>% 
#                 mutate(propensity_score = predict(prop.score.model, type = 'response'),
#                        treat = ifelse(versmodel_a == 'tele', 1, 0),
#                        weights_manual = (1 * treat) + ((propensity_score / (1 - propensity_score)) * (1 - treat))
#                        ) %>% 
#                 dplyr::select(-treat)
```

The weights generated using `WeightIt` and the manually generated weights are identical:

```{r, echo = FALSE}
# tibble(`Weights (WeightIt)` = res.weightit$weights, `Weights (manual)` = d.weights$weights_manual) %>% head()
```

Let's examine the weights again, whether there are any extreme weights which might adversely affect the standard errors or the estimation bias in the outcome model:

```{r}
# with(d.weights, by(weights_manual, versmodel_a, summary))
```

It doesn't seem like there are any extreme outliers, therefore truncation of the weights is redundant.

<br>

So now, instead of using the `cobalt:bal.tab()` function, lets calculate the (standardized) mean/ proportion differences also by hands using the function `mean.prop.diff()`. The rationale for doing so is partly because the results from the `cobalt` package are not compatible with the results of the `MatchIt` package (as mentioned earlier, the differences in proportions for binary or dummy coded categorical variables are not standardized when using `cobalt::bal.tab()`) and partly to make sure to understand how these difference-measures are calculated.

To calculate the (standardized) proportion differences, all categorical variables need to get dummy coded:

```{r}
# d.weights <- fastDummies::dummy_cols(d.weights, 
#                              select_columns = c('sex_c', 'sprgeb_x'))
```

First, the *unweighted* standardized mean differences for the continuous covariates and proportion differences for binary covariates (analogue to `cobalt::bal.tab()`):

```{r}
# # standardized mean differences for continuous variables
# diff.cont <- d.weights %>%
#   summarise(across(c(age_n, pre_bl_b, pre_kons_n), 
#                    ~ mean.prop.diff(data =  d.weights, variable = .x, treatment  = 'versmodel_a'))) %>% 
#   slice(2) %>% # extract 2 result of mean.prop.diff() corresponding to the standardized mean differences
#   t()  
# 
# # proportion differences (not standardized) for binary variables
# diff.binary <-  d.weights %>% 
#   summarise(across(c(pre_stat_fg, sex_c_m:`pre_pcg_cat_>= 6`), 
#                    ~ mean.prop.diff(data =  d.weights, variable = .x, treatment  = 'versmodel_a'))) %>% 
#   slice(1) %>% # extract 1 result of mean.prop.diff() corresponding to the raw proportion differences
#   t()  
# 
# rownames_to_column(data.frame(Diff = rbind(diff.cont, diff.binary)))
```

--\> Results are identical with the ones generated using the `bal.tab()` function from the `cobalt` package.

<br>

... and second, the *weighted* standardized mean differences for the continuous covariates and proportion differences for binary covariates (analogue to `cobalt::bal.tab()`):

```{r, message=FALSE}
# # weighted standardized mean differences for continuous variables
# w.diff.cont <-  d.weights %>% 
#   summarise(across(c(age_n, pre_bl_b, pre_kons_n), 
#                    ~ mean.prop.diff(data =  d.weights, variable = .x, treatment  = 'versmodel_a', weights = 'weights_manual'))) %>%
#   slice(2) %>% # extract 2 result of mean.prop.diff() corresponding to the standardized mean differences
#   t()  
# 
# # weighted proportion differences (not standardized) for binary variables
# w.diff.binary <-  d.weights %>% 
#   summarise(across(c(pre_stat_fg, sex_c_m:`pre_pcg_cat_>= 6`), 
#                    ~ mean.prop.diff(data =  d.weights, variable = .x, treatment  = 'versmodel_a', weights = 'weights_manual'))) %>%
#   slice(1) %>% # extract 1 result of mean.prop.diff() corresponding to the raw proportion differences
#   t()  
# 
# rownames_to_column(data.frame(`Diff weighted` = rbind(w.diff.cont, w.diff.binary)))
```

These results are identical with the ones generated using the `bal.tab()` function from the `cobalt` package. However, if the difference measure for the binary coded variable need to be comparable to the outputs of the `MatchIt` package, then the proportion differences in case of dummy coded variables should also be standardized. This is done for comparing the difference measure for the different propensity score based methods (see result section \@ref(comparison)).

Another option to compare the treatment and control group is using the `bal.stat()` function from the package `twang`. It is important to note, that the given proportion differences for binary coded variables are *standardized*, and thus compatible with the results from the `MatchIt` package, however, not with those of the function `cobalt::bal.tab()`.

```{r, echo=FALSE}
# covariate.binary <- d.weights %>% dplyr::select('sex_c_m':'pre_pcg_cat_>= 6') %>% names()
# covariateNames <- c('age_n', 'pre_bl_b', 'pre_kons_n', 'pre_stat_fg', covariate.binary)
# d.weights %<>% mutate(treatment = ifelse(versmodel_a == 'tele', 1, 0))
# 
# balanceTable <- twang::bal.stat(as.data.frame(d.weights), # needs to be a data frame, tibble does not work!
#                                 vars = covariateNames,
#                                 treat.var = 'treatment',
#                                 w.all = d.weights$weights_manual,
#                                 get.ks = F,
#                                 sampw = 1,
#                                 estimand = 'ATT',
#                                 multinom = F)
```

<br>

Also the following references and blog posts nicely explain how inverse probability of treatment weights can be calculated "manually" based on propensity scores, without using the `WeightIt` package:

-   Chapter 3 in Leite, W. Practical propensity score methods using R. Sage Publications, 2016.
-   https://www.franciscoyira.com/post/matching-in-r-3-propensity-score-iptw/#fn4
-   https://www.r-bloggers.com/2017/04/exploring-propensity-score-matching-and-weighting/

<br><br>

# Propensity score *stratification* using `MatchIt`: application {#stratification}

Propensity score *stratification* can be implemented in R using as well the package `MachtIt` with its function using the method *subclassification* as follows: `matchit(..., method = 'subclass')`.

```{r}
# # matchit object using the optimal matching method
# res.subclass <- MatchIt::matchit(versmodel_a ~ age_n + sex_c + sprgeb_x + pre_bl_b + pre_kons_n + pre_stat_fg + pre_atc_amb_cat + pre_pcg_cat,
#                               data = d.tele.roh,
#                               method = 'subclass',
#                               subclass = 5, # 5 subclasses
#                               estimand = 'ATT' # strata based on quintiles of the ps of the treated
#                               ) 
# 
# d.subclass <- MatchIt::match.data(res.subclass)
```

Check sample size in each stratum:

```{r}
# table(d.subclass$subclass, d.subclass$versmodel_a)
```

Using the argument `subclass = TRUE` within the `summary()` function gives the detailed "balance" statistics for all subclasses, in this particular case for all 5 subclasses.

```{r}
# smry.subclass <- summary(res.subclass, subclass = TRUE)
```

Extracting the summary statistics of individual classes (e.g. subclass 1):

```{r}
# smry.subclass$sum.subclass$`Subclass 1`[, 1:3]
```

<br><br>

# Comparison of standardized mean/proportion differences {#comparison}

Once a propensity score model has been defined and subsequent method have been applied, the next crucial step is to check on how well the compared groups are balanced in terms of the covariates. In case the differences, measure by the standardized mean differences, are still too big, the process of propensity score estimation should be repeated (e.g. using non-linear effects) until an adequate balance is achieved.

<br>

## Raw differences

```{r}
# 
# # raw data ---------------------------------------------------------------------
# smry.raw <- d.tele.roh %>% 
#    dplyr::select(versmodel_a, age_n, sex_c, sprgeb_x, pre_bl_b, pre_stat_fg, pre_kons_n, pre_pcg_cat, pre_atc_amb_cat) %>% 
#   gtsummary::tbl_summary(., by = 'versmodel_a') %>% 
#   modify_header(label ~ 'Variable', all_stat_cols() ~ "{level} N = {n}")
#    
# 
# # matched data -----------------------------------------------------------------
# smry.matched <- d.matchit %>% 
#   dplyr::select(versmodel_a, age_n, sex_c, sprgeb_x, pre_bl_b, pre_stat_fg, pre_kons_n, pre_pcg_cat, pre_atc_amb_cat) %>% 
#   gtsummary::tbl_summary(., by = 'versmodel_a') %>% 
#   modify_header(label ~ 'Variable', all_stat_cols() ~ "{level} N = {n}")
# tib.smry.matched <- as_tibble(smry.matched)
# 
# 
# # weighted data ----------------------------------------------------------------
# d.weights_smry <- d.weights %>% 
#                     dplyr::select(weights_manual, versmodel_a, age_n, sex_c, sprgeb_x, pre_bl_b, pre_stat_fg, pre_kons_n, pre_pcg_cat, pre_atc_amb_cat) 
# surveyDesign_smry <- survey::svydesign(ids = ~1, weights = ~weights_manual, data = d.weights_smry)
# smry.weighted <- surveyDesign_smry %>% 
#         gtsummary::tbl_svysummary(., by = 'versmodel_a') %>% 
#   modify_header(label ~ 'Variable', all_stat_cols() ~ "{level} N = {n}")
# tib.smry.weighted <- as_tibble(smry.weighted)
# 
# 
# # Stratification ---------------------------------------------------------------
# d.subclass_smry <- d.subclass %>% 
#   dplyr::select(subclass, versmodel_a, age_n, sex_c, sprgeb_x, pre_bl_b, pre_stat_fg, pre_kons_n, pre_pcg_cat, pre_atc_amb_cat) 
# tib.smry.subclass1 <- d.subclass_smry %>% 
#                         filter(subclass == 1) %>% dplyr::select(-subclass) %>% 
#                         gtsummary::tbl_summary(., by = 'versmodel_a') %>% 
#                         modify_header(label ~ 'Variable', all_stat_cols() ~ "{level} N = {n}") %>% 
#                         as_tibble()
# 
# tib.smry.subclass2 <- d.subclass_smry %>% 
#                         filter(subclass == 2) %>% dplyr::select(-subclass) %>% 
#                         gtsummary::tbl_summary(., by = 'versmodel_a') %>% 
#                         modify_header(label ~ 'Variable', all_stat_cols() ~ "{level} N = {n}") %>% 
#                         as_tibble()
# 
# tib.smry.subclass3 <- d.subclass_smry %>% 
#                         filter(subclass == 3) %>% dplyr::select(-subclass) %>% 
#                         gtsummary::tbl_summary(., by = 'versmodel_a') %>% 
#                         modify_header(label ~ 'Variable', all_stat_cols() ~ "{level} N = {n}") %>% 
#                         as_tibble()
# 
# tib.smry.subclass4 <- d.subclass_smry %>% 
#                         filter(subclass == 4) %>% dplyr::select(-subclass) %>% 
#                         gtsummary::tbl_summary(., by = 'versmodel_a') %>% 
#                         modify_header(label ~ 'Variable', all_stat_cols() ~ "{level} N = {n}") %>% 
#                         as_tibble()
# 
# tib.smry.subclass5 <- d.subclass_smry %>% 
#                         filter(subclass == 5) %>% dplyr::select(-subclass) %>% 
#                         gtsummary::tbl_summary(., by = 'versmodel_a') %>%
#                         modify_header(label ~ 'Variable', all_stat_cols() ~ "{level} N = {n}") %>% 
#                         as_tibble()
# 
# 
# # one table --------------------------------------------------------------------
# options(knitr.kable.NA = '')
# smry.raw %>% 
#   as_tibble() %>% 
#   left_join(., tib.smry.matched, by = 'Variable') %>% 
#   left_join(., tib.smry.weighted, by = 'Variable') %>%
#   left_join(., tib.smry.subclass1, by = 'Variable') %>%
#   #left_join(., tib.smry.subclass2, by = 'Variable') %>%
#   #left_join(., tib.smry.subclass3, by = 'Variable') %>%
#   #left_join(., tib.smry.subclass4, by = 'Variable') %>%
#   left_join(., tib.smry.subclass5, by = 'Variable') %>%
#   kable() %>% 
#   kable_styling(bootstrap_options = c("condensed", 'striped', 'hover', 'bordered'), 
#                 position = 'center', 
#                 full_width = FALSE) %>% 
#   add_header_above(c("", "Raw" = 2, "PS Matching" = 2, "PS Weighting" = 2, "Subclass 1" = 2, "Subclass 5" = 2)) %>% 
#   add_header_above(c("", " " = 6,
#                      "PS Stratification" = 4))
```

<br>

## Standardized differences

Based on the different methods applied above, the different *standardized mean/ proportion differences* between the treatment (tele) and control (standard) group are compiled in the following table:

```{r}

# # extract standardized mean/proportion differences for different methods
# 
# # raw (unmatched/unweighted) data
# std.m.d.raw <- rownames_to_column(data.frame(smry.nearest$sum.all))%>% 
#                       dplyr::select(Variable = rowname, `Raw data` = Std..Mean.Diff.)
# 
# # matched data
# std.m.d.nn <- rownames_to_column(data.frame(smry.nearest$sum.matched)) %>% 
#                       dplyr::select(Variable = rowname, `NN-Matching` = Std..Mean.Diff.)
# 
# 
# # weighted data
# std.m.d.weight <- d.weights %>% 
#   # calculate diff stats "by hand" to get numbers comparable to the outputs of matchit 
#   summarise(across(c(age_n, pre_kons_n, pre_bl_b, pre_stat_fg, sex_c_m:`pre_pcg_cat_>= 6`), ~ mean.prop.diff(data =  d.weights, 
#                                                               variable = .x, 
#                                                               treatment  = 'versmodel_a',
#                                                               weights = 'weights_manual'))) %>% 
#   slice(2) %>% # extract 2 result of mean.prop.diff() corresponding to the standardized differences for all
#   t() %>% 
#   data.frame(Weighting = .) %>% 
#   rownames_to_column(., var = 'Variable') %>% 
#   mutate(Weighting = unlist(unname(Weighting))) %>%
#   # adjust covariate names that they match with those from matchit-output
#   mutate(Variable = ifelse(Variable %like% 'age|pre_bl|pre_kons|pre_stat',
#                            Variable,
#                            stringi::stri_replace_last(Variable, '', regex = '_')
#                            )
#   )
# 
# 
# # stratified data
# std.m.d.s1 <- rownames_to_column(data.frame(smry.subclass$sum.subclass$`Subclass 1`)) %>% 
#                       dplyr::select(Variable = rowname, `Quintile 1` = Std..Mean.Diff.)
# std.m.d.s2 <- rownames_to_column(data.frame(smry.subclass$sum.subclass$`Subclass 2`)) %>% 
#                       dplyr::select(Variable = rowname, `Quintile 2` = Std..Mean.Diff.)
# std.m.d.s3 <- rownames_to_column(data.frame(smry.subclass$sum.subclass$`Subclass 3`)) %>% 
#                       dplyr::select(Variable = rowname, `Quintile 3` = Std..Mean.Diff.)
# std.m.d.s4 <- rownames_to_column(data.frame(smry.subclass$sum.subclass$`Subclass 4`)) %>% 
#                       dplyr::select(Variable = rowname, `Quintile 4` = Std..Mean.Diff.)
# std.m.d.s5 <- rownames_to_column(data.frame(smry.subclass$sum.subclass$`Subclass 5`)) %>% 
#                       dplyr::select(Variable = rowname, `Quintile 5` = Std..Mean.Diff.)
# 
# 
# # make nice table
# std.m.d.all <- std.m.d.raw %>% 
#                   left_join(std.m.d.nn, by = 'Variable') %>% 
#                   left_join(std.m.d.weight, by = 'Variable') %>% 
#                   left_join(std.m.d.s1, by = 'Variable') %>% 
#                   left_join(std.m.d.s2, by = 'Variable') %>% 
#                   left_join(std.m.d.s3, by = 'Variable') %>% 
#                   left_join(std.m.d.s4, by = 'Variable') %>% 
#                   left_join(std.m.d.s5, by = 'Variable') %>%
#                   filter(Variable != 'distance') # remove stats on propensity scores
# 
# # nice table
# std.m.d.all %>% 
#   mutate(across(c(`Raw data`: `Quintile 5`), red_cell_spec)) %>% 
#   kable(escape = FALSE) %>% 
#   kable_styling(bootstrap_options = c("condensed", 'striped', 'hover', 'bordered'), 
#                 position = 'center', 
#                 full_width = FALSE) %>% 
#   add_header_above(c("", "None" = 1, 
#                      "PSM (a)" = 1,  # paste0("PSM", footnote_marker_alphabet(1))
#                      "PSW (b)" = 1,
#                      "PSS (c)" = 5)) %>%  # paste0("PSS", footnote_marker_alphabet(2))
#   footnote(general = 'For continuous variables standardized mean differences are calculated based on equation (1), and for dummy coded categorical variables standardized proportion differences based on equation (2). Numbers marked in red indicate standardized mean/ proportion differences > 0.05 or < -0.05.',
#            alphabet = c("PSM = Propensity Score Matching", 
#                         "PSW = Propensity Score Weighting",
#                         "PSS = Propensity Score Stratification"),
#            general_title = ""
#            )
# 
# ordered_covariates <- std.m.d.all %>% 
#                           dplyr::select(Variable, `Raw data`) %>% 
#                           mutate(`Raw data` = abs(`Raw data`)) %>% 
#                           arrange(`Raw data`) %>% pull(Variable)
# 
# # nice plot
# std.m.d.all %>%
#   pivot_longer(., cols = c(2:9), names_to = 'Method') %>% 
#   mutate(value = abs(value),
#          Variable = factor(Variable, levels = ordered_covariates),
#          class = factor(case_when(Method %in% c('Raw data') ~ 'Raw data',
#                            Method %in% c('NN-Matching', 'Weighting') ~ 'Matching/ Weighting',
#                            TRUE ~ 'Stratification'), 
#                         levels = c('Raw data', 'Matching/ Weighting', 'Stratification')),
#          Method = factor(Method, levels = c('Raw data', 'NN-Matching', 'Weighting', 
#                                         'Quintile 1', 'Quintile 2', 'Quintile 3', 'Quintile 4', 'Quintile 5'))) %>% 
#   ggplot(., aes(x = Variable, y = value, group = Method, color = Method)) + 
#   geom_point(size = 2) + 
#   geom_line() +
#   coord_flip() +
#   geom_hline(yintercept = c(0.05), linetype="dashed") +
#   facet_wrap(~class, scales = 'fixed') +
#   ylim(-0.05, 0.7) + 
#   ylab('Standardized Mean Difference') + 
#   theme_bw(base_size = 14) +
#   theme(panel.grid.major = element_blank(), 
#         panel.grid.minor = element_blank(),
#         panel.background = element_blank(),
#         axis.line = element_line(colour = "black"))
```

Comparing the standardized mean/ proportion differences of the different propensity score based methods shows that especially before any balancing approach is applied, the covariates show great inequality (common rule of thumb: \> 0.05 \| \< -0.05) between the treatment and control group. However, these differences vanish using propensity score matching and weighting. In case of propensity score stratification, the propensity score quintiles show quite some inbalance still.

<br><br>

# Estimation of odds ratios and confidence intervals

Estimating the actual (treatment) effects, and its uncertainty in form of standard errors and confidence intervals - after modelling propensity scores and assessing balance - is a science of its own. The results and particularly the standard errors, and confidence intervals, respectively, of the subsequent outcome model (e.g. logistic regression) may be biased due to various reasons.

-   Matching: "Specifically, a clustering variable is added to the regression model to identify the five non-cancer controls who are matched to each cancer case. Cases and controls within the same matched cluster have similar propensity scores; thus, they are, on average, more similar than are randomly selected cancer and non-cancer respondents. Since the matched samples are not independent, statistical analyses must adjust for the matched nature of the design (Austin, 2008)." ([Reeve et al. 2008](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4195028/))

-   Weighting: Downweighting and Upweighting lowers estimation precision (see: )

-   Stratification: study subjects within same strata are more similar to oneanother

In principle, there are two approaches to adequately measure the variance of the estimators: (1) using analytical formulas and (2) bootstrapping. Even though, bootstrapping is a simple method which is able to take into account uncertainties of the whole process (propensity score model as well as outcome model), it is also computationally very expensive. Considering the large amount of data in the given example, the former method seems therefore more suitable. The recommended procedure of estimation using the analytical approach very much depends on the propensity score method used and its specifications (e.g. matching with/without replacement). I followed the procedures presented in the [vignette](https://cran.r-project.org/web/packages/MatchIt/vignettes/estimating-effects.html) of the `MatchIt` package and in Leite 2016.

```{r}
# Estimate treatment effects (using jtools to get robust SEs)
# alternative ways compared to survey package
# from https://stats.stackexchange.com/questions/405019/matching-with-multiple-treatments
# library(jtools)
# summ(glm(outcome ~ relevel(treat, "control"), data = data,
#          weights = w.out$weights), robust = "HC1")
```

```{r}
# # **Multiple regression based on raw data**
# formula <- 'qi ~ age_n + sex_c + sprgeb_x + pre_bl_b + pre_kons_n + pre_stat_fg + pre_atc_amb_cat + pre_pcg_cat + versmodel_a'
# 
# # (1) multiple regression based on raw data
# glm.raw <- glm(formula, data = d.tele.roh, family = 'binomial')
# OR_CI_reg <- OR_CI_fct(glm.raw, 'versmodel_atele')
```

```{r}
# # **Multiple regression based on nn-matched data**
# # (2) multiple regression based on nn-matched data
# glm.matched <- glm(formula, data = d.matchit, family = 'binomial') # weights not included becaus matched 1:1 without replacement
# #summary(glm.matched)
# 
# # unadjusted OR and CI (might be biased)
# #OR_CI_matched <- OR_CI_fct(glm.matched, 'versmodel_atele')
# 
# comp.matched <- marginaleffects::comparisons(glm.matched,
#                                              variables = 'versmodel_a',
#                                              vcov = ~subclass,           # request a cluster robust se
#                                              newdata = subset(d.matchit, versmodel_a == 'tele'), # request ATT
#                                              transform_pre = "lnoravg" # Population-averaged (marginal) odds ratios
#                                              )
# 
# # get cluster robust S, CI, respectively:
# # details see: https://cran.r-project.org/web/packages/MatchIt/vignettes/estimating-effects.html
# smry.matched <- summary(comp.matched, transform_avg = exp) # gets the exp() of the effect --> OR
# OR_CI_matched_robust <- c(smry.matched$estimate, smry.matched$conf.low, smry.matched$conf.high)
```

```{r, echo = FALSE}
# # **Multiple regression based on weighted data**
# # (3) weighted multiple regression
# #glm.weighted <- glm(formula, data = d.weights, family = 'binomial', weights = weights_manual)
# # unadjusted OR and CI (non-robust method)
# # OR_CI_weighted <- OR_CI_fct(glm.weighted, 'versmodel_atele')
# # here important: standard errors should be robust
# # see e.g.: https://www.r-bloggers.com/2017/04/exploring-propensity-score-matching-and-weighting/ 
# 
# # robust estimates of SE, and CI, respectively
# # gives warning message --> 'Unable to extract a variance-covariance matrix using this `vcov`'
# # comp.weighted <- marginaleffects::comparisons(glm.weighted,
# #                                               variables = 'versmodel_a',
# #                                               vcov = ~subclass,           # request a cluster robust se
# #                                               newdata = subset(d.weights, versmodel_a == 1), # request ATT
# #                                               wts = 'weights_manual',
# #                                               transform_pre = "lnoravg" # Population-averaged (marginal) odds ratios
# #                                               )
# 
# 
# # getting robust estimate using survey package
# surveyDesign <- survey::svydesign(ids = ~1, weights = ~weights_manual, data = d.weights)
# outcomeModel <- survey::svyglm(formula, surveyDesign, family=binomial)
# OR_CI_weighted_robust <- rownames_to_column(data.frame(summary(outcomeModel)$coefficients)) %>% 
#                           filter(rowname == 'versmodel_atele') %>% 
#                           transmute(or = exp(Estimate), 
#                                     or.lci = exp(Estimate - 1.96*Std..Error),
#                                     or.uci = exp(Estimate + 1.96*Std..Error))
```

```{r, echo = FALSE}
# # **Multiple regression for stratified data**
# # Here I used the procedure for marginal mean weighting through stratification (MMWS) as described in Leite 2016.
# 
# # (4) for each stratified subclass multiple regression
# 
# # unadjusted or and ci for individual strati
# OR_CI_subclass <- d.subclass %>% 
#                     nest(-subclass) %>% 
#                     mutate(fit = map(data, ~ glm(formula, data = ., family = 'binomial')),
#                            tidied = map(fit, tidy)) %>% 
#                     unnest(tidied) %>% 
#                     filter(term == 'versmodel_atele') %>% 
#                     transmute(Typ = paste0('Subclass ', subclass),
#                               or = round(exp(estimate),5),
#                               or.lci = exp(estimate-1.96*std.error),
#                               or.uci = exp(estimate+1.96*std.error)) %>% 
#                     arrange(Typ) %>% data.frame()
# 
# d.subclass.s <- d.subclass %>% dplyr::select(qi, subclass, age_n, sex_c, sprgeb_x, pre_bl_b, pre_kons_n, pre_stat_fg, pre_atc_amb_cat, pre_pcg_cat, versmodel_a) %>% 
#   mutate(treat = ifelse(versmodel_a == 'tele', 1, 0))
# 
# # use stratum weights to estimate a single average marginal effect
# # subclass specific approach (omits weights and uses subclasses directly)
# # glm.subclass <- glm(qi ~ subclass * (treat + age_n + sex_c + sprgeb_x + pre_bl_b + pre_kons_n + pre_stat_fg + pre_atc_amb_cat + pre_pcg_cat), data = d.subclass.s, family = 'binomial')
# # 
# # # following does not work --> warning: vcov unable to extract vcov
# # comp.subclass <- marginaleffects::comparisons(glm.subclass,
# #                                               variables = 'treat',
# #                                               vcov = "HC3", # for marginal mean through stratification robust instead of cluster robust SE
# #                                               newdata = subset(d.subclass.s, treat == 1), # request ATT
# #                                               transform_pre = "lnoravg" # Population-averaged (marginal) odds ratios
# #                                               )
# #summary(comp.subclass, transform_avg = exp) # gets the exp() of the effect --> OR
# 
# 
# # using survey
# subclassTreat <- data.frame(table(d.subclass$subclass[d.subclass$versmodel_a=='tele']))
# names(subclassTreat) <- c('subclass','N.1s')
# 
# subclassUntreat <- data.frame(table(d.subclass$subclass[d.subclass$versmodel_a=='standard']))
# names(subclassUntreat) <- c('subclass','N.0s')
# 
# (table.subclass <- merge(subclassTreat, subclassUntreat))
# 
# d.subclass <- merge(d.subclass, table.subclass)
# surveyDesign <- survey::svydesign(ids = ~1, weights = 1, data = d.subclass)
# (prop.treat <- survey::svymean(~versmodel_a, surveyDesign))
# 
# d.subclass$mmwsATT <- with(d.subclass, ifelse(versmodel_a=='tele', 1, N.1s*prop.treat[1]/N.0s*prop.treat[2]))
# xtabs(~mmwsATT+subclass, d.subclass)
# 
# d.subclass$mmwsATT <- d.subclass$mmwsATT/mean(d.subclass$mmwsATT)
# 
# 
# surveyDesignATT <- svydesign(ids = ~1, weights = ~mmwsATT, data = d.subclass)
# modelATT <- survey::svyglm(formula, surveyDesignATT, family = 'binomial')
# OR_CI_subclass_robust <- rownames_to_column(data.frame(summary(modelATT)$coefficients)) %>% 
#                           filter(rowname == 'versmodel_atele') %>% 
#                           transmute(or = exp(Estimate), 
#                                     or.lci = exp(Estimate - 1.96*Std..Error),
#                                     or.uci = exp(Estimate + 1.96*Std..Error))
```

```{r}
# # one data frame
# 
# # for unadjusted OR
# cont.tab <- table(d.tele.roh$versmodel_a, d.tele.roh$qi)
# OR_CI_raw <- oddsratio(cont.tab)$measure[2,]
# names(OR_CI_raw) <- c('or', 'or.lci', 'or.uci')
# 
# OR_CI <- rbind(c('Raw', OR_CI_raw),
#                c('Regr. adjustment', OR_CI_reg),
#                c('Matched', OR_CI_matched_robust),
#                c('Weighted', OR_CI_weighted_robust),
#                c('Stratified', OR_CI_subclass_robust)
#                )
# OR_CI <- data.frame(OR_CI) %>% 
#             rename(Typ = 'V1')
# 
# d.plot <- #rbind(OR_CI, OR_CI_subclass) %>% 
#            OR_CI %>% 
#             mutate(across(Typ:or.uci, ~unlist(unname(.x)))) %>% 
#             mutate(across(or:or.uci, ~as.numeric(.x))) %>% 
#             mutate(Typ = factor(Typ, levels = c('Raw', 'Regr. adjustment', 'Matched', 'Weighted', 'Stratified')))
# 
# d.plot %>% 
#   ggplot(., aes(y = or, x = Typ)) +
#   geom_point(position = position_dodge(width = 0.7), size=2, show.legend = F) +
#   geom_errorbar(aes(ymin=or.lci, ymax=or.uci), width = 0.1, size = 0.75, position = position_dodge(width = 0.7)) +
#   geom_hline(aes(yintercept = 1), color = 'grey80', lty = 2, size = 0.8, show.legend = F) +
#   scale_x_discrete(limits=rev) +
#   coord_flip() +
#   xlab('') + ylab('OR \u00B1 CI') +
#   ggtitle('PIM - Telmed vs. Standard') +
#   theme_bw(base_size = 14) +
#   theme(legend.title = element_text(size = 10),
#         legend.text = element_text(size = 10))
```

<br><br>

# Multiple Treatments PIM: Propensity Score Weighting

## Multinomial Propensity Score Model

```{r}
# ps.formula <- 'versmodel_a ~ age_n + sex_c + sprgeb_x + pre_bl_b + pre_kons_n + pre_stat_fg + pre_atc_amb_cat + pre_pcg_cat'
# 
# W.out <- WeightIt::weightit(as.formula(ps.formula), data = d.qi.pim, estimand = "ATT", method = "ps", focal = 'contract')
```

<br>

## Inverse probability treatment weights

```{r}
# summary(W.out)
# 
# summary(W.out$weights)
```

<br>

## Balance Assessment

```{r}
# cobalt::bal.tab(W.out, stats = c("m", "v"), thresholds = c(m = .05))
```

<br>

## Weighted Regresssion Analyses

```{r}
# outcome.formula <- 'qi ~ age_n + sex_c + sprgeb_x + pre_bl_b + pre_kons_n + pre_stat_fg + pre_atc_amb_cat + pre_pcg_cat + versmodel_a'
# 
# # trim weights at 90th percentile --> does hardly effect the treatment effects!
# # trim.tresh <- quantile(W.out$weights, probs = seq(0, 1, 0.1))[10]
# # trim.weights <- ifelse(W.out$weights > trim.tresh, trim.tresh, W.out$weights)
# 
# # estimate effect
# d.q <- survey::svydesign(~1, weights = W.out$weights, data = d.qi.pim)
# fit <- survey::svyglm(as.formula(outcome.formula), design = d.q, family = binomial)
# summary(fit)
# contrasts_mcp(fit) # function from 00_QI_versmodel_functions.R
```

<br><br>

# References

-   Austin PC. An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies. Multivariate Behav Res. 2011 May;46(3):399-424. doi: 10.1080/00273171.2011.568786. Epub 2011 Jun 8. PMID: 21818162; PMCID: PMC3144483.

-   Amoah J, Stuart EA, Cosgrove SE, Harris AD, Han JH, Lautenbach E, Tamma PD. Comparing Propensity Score Methods Versus Traditional Regression Analysis for the Evaluation of Observational Data: A Case Study Evaluating the Treatment of Gram-Negative Bloodstream Infections. Clin Infect Dis. 2020 Dec 3;71(9):e497-e505. doi: 10.1093/cid/ciaa169. PMID: 32069360; PMCID: PMC7713675.

-   Shah BR, Laupacis A, Hux JE, Austin PC. Propensity score methods gave similar results to traditional regression modeling in observational studies: a systematic review. J Clin Epidemiol. 2005 Jun;58(6):550-9. doi: 10.1016/j.jclinepi.2004.10.016. Epub 2005 Apr 19. PMID: 15878468.

-   Elze MC, Gregson J, Baber U, Williamson E, Sartori S, Mehran R, et al. Comparison of propensity score methods and covariate adjustment: evaluation in 4 cardiovascular studies. J Am Coll Cardiol 2017; 69:345e57.

-   Leite, W. Practical propensity score methods using R. Sage Publications, 2016.

-   Ho, D., Imai, K., King, G., & Stuart, E. A. (2011). MatchIt: Nonparametric Preprocessing for Parametric Causal Inference.Journal of Statistical Software,42(8), 1–28. https://doi.org/10.18637/jss.v042.i08

    -   https://cran.r-project.org/web/packages/WeightIt/vignettes/WeightIt.html#ref-austinIntroductionPropensityScore2011

    -   https://cran.r-project.org/web/packages/MatchIt/vignettes/estimating-effects.html

-   https://cran.r-project.org/web/packages/WeightIt/vignettes/WeightIt.html
